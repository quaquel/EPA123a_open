{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The evolution of cooperation\n",
    "\n",
    "Why do members of the same species often cooperate? Why does it seem that\n",
    "cooperation is evolutionarily beneficial, and how might this have come about? The economist and nobel laureate Robert Axelrod worked on these questions in the late 1970s and early 1980s. To investigate this question, he set up the following computer experiment. He investigated the performance of different strategies for playing the iterated prisoners dilemma. He set up a tournament where he invited colleagues to submit strategies. He informed them that the strategies would play the iterated prisoners dilemma for an unknown number of iterations. The aim was to design a strategy that would collect the most points while playing against all other submitted strategies. Some strategies that were submitted are\n",
    "\n",
    "* **tit for tat** play nice, unless, in the previous move, the other player betrayed you.\n",
    "* **contrite tit for tat**; play nice, unless, in the previous two moves, the other player betrayed you.\n",
    "* **grim trigger** play nice until the other player betrays you after which you will always defect as well.\n",
    "* **pavlov**; there is an entire family of pavlov strategies. The basic idea is that this strategy sticks to what was successful. The simplest version is if my and opponent move were the same last time, stay, else switch.\n",
    "* **always defect**;\n",
    "* **always cooperate**;\n",
    "\n",
    "More details and variations on this can be found in \n",
    "\n",
    "* [Axelrod & Hamilton (1981) The evolution of cooperation, Science, Vol. 211, No. 4489](http://www-personal.umich.edu/~axe/research/Axelrod%20and%20Hamilton%20EC%201981.pdf)\n",
    "* [Axelrod, Robert (1984), The Evolution of Cooperation, Basic Books, ISBN 0-465-02122-0](https://www.amazon.com/Evolution-Cooperation-Revised-Robert-Axelrod/dp/0465005640)\n",
    "\n",
    "In this assignment, step by step, we are going to built a model for exploring how these different strategies perform when playing the iterated prisoners dilemma. In the first version, a set of strategies plays the iterated prisoner dilemma game against all other strategies, and we tally up the total scores. After that, we are going to slowly expand this model. First by adding some randomness to it. This is something that is very common in agent based modeling. Second, we are going to make the model dynamic by adding a small evolutionary mechanism to it. Third, we are going to combine the randomness and evolutionary dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a first model \n",
    "Below, we give the initial code you will be expanding on while developing\n",
    "models of the emergence of cooperation. You can look at the code block below, or scroll further down where I explain this code in more detail.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from enum import Enum\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from mesa import Model, Agent\n",
    "\n",
    "\n",
    "class Move(Enum):\n",
    "    COOPERATE = 1\n",
    "    DEFECT = 2\n",
    "\n",
    "\n",
    "class AxelrodAgent(Agent):\n",
    "    \"\"\" An agent with fixed initial wealth.\"\"\"\n",
    "    def __init__(self, model, n_rounds, noise_level=0): \n",
    "        super().__init__(model)\n",
    "        self.n_rounds = n_rounds\n",
    "        self.points = 0\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Playing the iterated prisoners dilemma agains all other agents in the model\"\"\"\n",
    "        for other in self.model.agents:\n",
    "            if other is self:\n",
    "                # we don't play against ourselves\n",
    "                continue\n",
    "            for _ in range(self.n_rounds):\n",
    "                move_a = self.move()\n",
    "                move_b = other.move()\n",
    "\n",
    "                #insert noise in movement, we\n",
    "                if self.random.random() < self.noise_level:\n",
    "                    if move_a == Move.COOPERATE:\n",
    "                        move_a = Move.DEFECT\n",
    "                    else:\n",
    "                        move_a = Move.COOPERATE\n",
    "                if self.random.random() < self.noise_level:\n",
    "                    if move_b == Move.COOPERATE:\n",
    "                        move_b = Move.DEFECT\n",
    "                    else:\n",
    "                        move_b = Move.COOPERATE                \n",
    "                \n",
    "                payoff_a, payoff_b = self.model.payoff_matrix[(move_a, move_b)]\n",
    "                \n",
    "                self.receive_payoff(payoff_a, move_a, move_b)\n",
    "                other.receive_payoff(payoff_b, move_b, move_a)\n",
    "            self.reset()\n",
    "            other.reset()\n",
    "\n",
    "    def move(self):\n",
    "        \"\"\"The move to make in this iteration of the game\n",
    "        \n",
    "        Returns:\n",
    "            Move.COOPERATE or Move.DEFECT\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        \"\"\"Receive payoff and moves resulting in that payoff.\n",
    "\n",
    "        Args:\n",
    "            payoff : int\n",
    "            my_move : {Move.COOPERATE, Move.DEFECT}\n",
    "            opponents_move : {Move.COOPERATE, Move.DEFECT}\n",
    "\n",
    "        \"\"\"\n",
    "        self.points += payoff\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"reset agent after completing playing agains another agent\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class TitForTat(AxelrodAgent):\n",
    "    \n",
    "    def __init__(self, model, n_rounds, noise_level=0): \n",
    "        super().__init__(model, n_rounds, noise_level=noise_level) \n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "    \n",
    "    def move(self):\n",
    "        return self.opponent_last_move\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_move = opponent_move\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "\n",
    "\n",
    "class ContriteTitForTat(AxelrodAgent):\n",
    "    \n",
    "    def __init__(self, model, n_rounds, noise_level=0): \n",
    "        super().__init__(model, n_rounds, noise_level=noise_level) \n",
    "        self.opponent_last_two_moves = deque([Move.COOPERATE, Move.COOPERATE], maxlen=2)\n",
    "    \n",
    "    def move(self):\n",
    "        if (self.opponent_last_two_moves[0] == Move.DEFECT) and\\\n",
    "           (self.opponent_last_two_moves[1] == Move.DEFECT):\n",
    "            return Move.DEFECT\n",
    "        else:\n",
    "            return Move.COOPERATE\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_two_moves.append(opponent_move)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_two_moves = deque([Move.COOPERATE, Move.COOPERATE], maxlen=2)        \n",
    "\n",
    "\n",
    "class AxelrodModel(Model):\n",
    "    \"\"\"A model with some number of agents.\"\"\"\n",
    "    def __init__(self, n, seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "        self.payoff_matrix = {\n",
    "            (Move.COOPERATE, Move.COOPERATE):(2, 2),\n",
    "            (Move.COOPERATE, Move.DEFECT): (0, 3),\n",
    "            (Move.DEFECT, Move.COOPERATE): (3, 0),\n",
    "            (Move.DEFECT, Move.DEFECT): (1, 1)\n",
    "        }\n",
    "        \n",
    "        # Create agents\n",
    "        for agent_class in AxelrodAgent.__subclasses__():\n",
    "            a = agent_class(self, n) \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Advance the model by one step.\"\"\"\n",
    "        self.agents.do(\"step\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code gives the basic setup for a first version of a model of the evolution of cooperation. Let's quickly walk through this code. \n",
    "\n",
    "We begin with a number of import statements. We import the ``deque`` class\n",
    "from the ``collections`` library. Deque is basically a pipeline of fixed\n",
    "length. We put stuff in at one end, and stuff drops of at the other end. We\n",
    "use the deque to create a memory of previous moves of a given length. See\n",
    "the ``ContriteTitForTat`` class for how we use it. Next, we import the\n",
    "``Enum`` class from the ``enum`` library. Enums are a way of specifying a\n",
    "fixed number of unique names and associated values. We use it to standardize\n",
    " the 2 possible moves Agents can make. Next, we import the ``combinations`` function from the ``itertools`` library. We use this function to pair off all agents against one another. See the ``step`` method in the ``AxelrodModel`` class. The Python programming language comes with many useful libraries. It is well worth spending some time reading the detailed documentation for in particular itertools and collections. Many common tasks can readily be accomplished by using these libraries.\n",
    "\n",
    "Next, we move to importing the ``Model`` and ``Agent`` classes from MESA. Agent is the base class from which all agents in a model have to be derived. Similarly, Model is the base class from which any given model is derived. \n",
    "\n",
    "Next, I have defined a generic ``AxelrodAgent``. Let's look at this class in a bit more detail starting with the first line\n",
    "\n",
    "```python\n",
    "class AxelrodAgent(Agent):\n",
    "```\n",
    "\n",
    "The word ``class`` is like ``def`` in that it indicates that we are describing something that can be used later. Here we are defining a class, which we can use by instantiating it as an object. We call this class ``AxelrodAgent`` and it extends (i.e. is a further detailing of) the base ``Agent`` class that we imported from Mesa.\n",
    "\n",
    "This AxelrodAgent has 4 methods\n",
    "\n",
    "```python\n",
    "    def __init__(self, model, n_rounds, noise_level=0): \n",
    "        ...\n",
    "\n",
    "    def step(self):\n",
    "        ...\n",
    "\n",
    "    def move(self):\n",
    "        ...\n",
    "\n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "         ...\n",
    "        \n",
    "    def reset(self):\n",
    "        ...\n",
    "\n",
    "```\n",
    "Any method in Python has as its first variable ``self``. This is not\n",
    "something that you need to pass when calling the method. It is something automatically inserted by Python. Self refers to this specific instance of the class (i.e., the object), and it allows you to assign values to it or call methods on itself.\n",
    "\n",
    "The first method, ``__init__`` is common to any Python class. This method is called when instantiating the class as an object. The first variable in the ``__init__``, ``model``, is expected by MESA. `n_rounds` and `noise_level` are an argument and keyword argument respectively that we'll be using in this assignment. The ``step`` method is also expected by Mesa. The other three methods `move`, ``receive_payoff`` and ``reset``,have been chosen by me. Note how we are specifying, implicitly, a pattern of interaction. Each of these methods is called under specific conditions and does something to the state of the agent (make a move in one round of the iterated prisoners dilemma, receive payoff, and reset). `move` is called for each iteration of a single instance of the iterated prisoners dilemma. ``receive_payoff`` is called after each round of the prisoners dilemma. ``reset`` is called after having played the iterated prisoners dilemma against another strategy for `n_rounds`.\n",
    "\n",
    "This pattern of interaction is captured in `AxelrodAgent.step`. Here, we specify that a given agent is playing the iterated prisoners dilmema against all other agents in the model. Of course, an agent won't play against itself, so we check fo this and skip it. Given another agent, the game is played for `n_rounds`. Both agents make their move. Next, there is a small code block to insert noise (i.e., flip the intended move) that will be used later. Because by default `noise_level=0`, this code blcok is effectively skipped. Given the actual moves, we can get the payoff, and inform both agents of this payoff and the other agent's move. Once this game has been played for `n_rounds`, both players are reset for a next game. \n",
    "\n",
    "```python\n",
    "    def step(self):\n",
    "        \"\"\"Playing the iterated prisoners dilemma agains all other agents in the model\"\"\"\n",
    "        for other in self.model.agents:\n",
    "            if other is self:\n",
    "                # we don't play against ourselves\n",
    "                continue\n",
    "            for _ in range(self.n_rounds):\n",
    "                move_a = self.move()\n",
    "                move_b = other.move()\n",
    "\n",
    "                #insert noise in movement, we\n",
    "                if self.random.random() < self.noise_level:\n",
    "                    if move_a == Move.COOPERATE:\n",
    "                        move_a = Move.DEFECT\n",
    "                    else:\n",
    "                        move_a = Move.COOPERATE\n",
    "                if self.random.random() < self.noise_level:\n",
    "                    if move_b == Move.COOPERATE:\n",
    "                        move_b = Move.DEFECT\n",
    "                    else:\n",
    "                        move_b = Move.COOPERATE                \n",
    "                \n",
    "                payoff_a, payoff_b = self.model.payoff_matrix[(move_a, move_b)]\n",
    "                \n",
    "                self.receive_payoff(payoff_a, move_a, move_b)\n",
    "                other.receive_payoff(payoff_b, move_b, move_a)\n",
    "            self.reset()\n",
    "            other.reset()\n",
    "\n",
    "```\n",
    "\n",
    "Of the 5 methods, 3 are implemented and 2 raise an error. Any specific strategy class that we are going to implement thus needs to implement always at least the move and reset method, while it can rely on the behavior of ``__init__`` and ``receive_payoff``, extend this behavior, or overwrite it. Let's look at these three options in some more detail.\n",
    "\n",
    "If a subclass of AxelrodAgent does not implement either ``__init__`` and ``receive_payoff``, it automatically falls back on using the behavior specified in the AxelrodAgent class. We can also extend the behavior. For this look at the ``TitForTat`` class:\n",
    "\n",
    "```python\n",
    "class TitForTat(AxelrodAgent):\n",
    "    \n",
    "    def __init__(self, model, n_rounds, noise_level=0): \n",
    "        super().__init__(model, n_rounds, noise_level=noise_level) \n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "    \n",
    "    def move(self):\n",
    "        return self.opponent_last_move\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_move = opponent_move\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "```\n",
    "\n",
    "Note how both ``__init__`` and ``receive_payoff`` start with ``super()``. This means that we first call the same method on the parent class (so ``AxelrodAgent``). Next we have some additional things we want to do. In ``__init__`` we create a novel attribute ``opponent_last_move``, which we set to ``Move.COOPERATE``. Note how we use the ``self`` variable. In receive_payoff, we update this attribute. Finally, we can overwrite the entire implementation of a method. For this, all we need to do is not call super."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: implementing your first strategies\n",
    "Before looking at the model class more closely, Implement the following strategies as classes (in order of easy to difficult)\n",
    "\n",
    "* **Defector**; always defect\n",
    "* **Cooperator**; always cooperate\n",
    "* **GrimTrigger**; cooperate until betrayed, after which it always defects\n",
    "* **Pavlov**; The basic idea is that this strategy sticks to what was successful. The simplest version is if my and opponent move were the same last time, stay, else switch. Pavlov always starts assuming in the previous move, both agents played ``Move.COOPERATE``\n",
    "\n",
    "To help you, I have given you the basic template and all you need to do is write some code replacing the dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defector(AxelrodAgent):\n",
    "    def move(self):\n",
    "        ...\n",
    "    \n",
    "    def reset(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "class Cooperator(AxelrodAgent):\n",
    "    def move(self):\n",
    "        ...\n",
    "    \n",
    "    def reset(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "class GrimTrigger(AxelrodAgent):\n",
    "    def __init__(self, model, n_rounds, noise_level=0): \n",
    "        super().__init__(model, n_rounds, noise_level=noise_level) \n",
    "        ...\n",
    "    \n",
    "    def move(self):\n",
    "        ...\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        ...\n",
    "        \n",
    "    def reset(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "class Pavlov(AxelrodAgent):\n",
    "    def __init__(self, model, n_rounds, noise_level=0): \n",
    "        super().__init__(model, n_rounds, noise_level=noise_level) \n",
    "        ...      \n",
    "    \n",
    "    def move(self):\n",
    "        ...\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        ...\n",
    "        \n",
    "    def reset(self):\n",
    "        ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model, let's quickly walk through the code of the model class.\n",
    "\n",
    "```python\n",
    "class AxelrodModel(Model):\n",
    "```\n",
    "\n",
    "So, here we define a new model class which extends the default model class from MESA. This class typically has at least 2 methods: ``__init__`` and ``step``. In the init we instantiate the model, while in step we specify what should happen in one step of the model. A step, or tick, is something particular to Agent Based Models. A step generally involves allowing all agents to take an action (i.e. you call step on all agents). \n",
    "\n",
    "Lets' look more closely at the init\n",
    "\n",
    "```python\n",
    "def __init__(self, N, seed=None):\n",
    "    super().__init__(seed=seed)\n",
    "    self.num_iterations = N\n",
    "    self.payoff_matrix = {}\n",
    "\n",
    "    self.payoff_matrix[(Move.COOPERATE, Move.COOPERATE)] = (2, 2)\n",
    "    self.payoff_matrix[(Move.COOPERATE, Move.DEFECT)] = (0, 3)\n",
    "    self.payoff_matrix[(Move.DEFECT, Move.COOPERATE)] = (3, 0)\n",
    "    self.payoff_matrix[(Move.DEFECT, Move.DEFECT)] = (1, 1)\n",
    "\n",
    "    # Create agents\n",
    "    for agent_class in AxelrodAgent.__subclasses__():\n",
    "        a = agent_class(self)\n",
    "```\n",
    "\n",
    "we are extending the default ``__init__`` from ``Model``, so we call\n",
    "``super`` first. Next, we specify the model's attribute: the payoff matrix. Note that a high payoff is desirable. Next we populate the model with one instance of each type of strategy.\n",
    "\n",
    "```python\n",
    "for agent_class in AxelrodAgent.__subclasses__():\n",
    "```\n",
    "\n",
    "``AxelrodAgent.__subclasses__()`` is a python *magic* method as indicated by the leading and trailing double underscore. Moreover, this is a so-called class method. Remember, when introducing object oriented programming, I said that methods are tied to objects (i.e., instances of the class). This is 95% true, but it is possible to define methods (and attributes) at the class level as well. These are useful for doing tasks that don't rely on the state of the object but are relevant to the behavior of the class. Don't worry too much about getting your head around this. It is a rather advanced and esoteric topic that we don't need to worry about too much. Here, ``__subclasses__`` returns a list with all classes that extend ``AxelrodAgent``. That is, all the different strategies we hve been defining. Note that this also showcases a benefit of using Object Orientation. If we implement new strategies, as long as they extend ``AxelrodAgent``, we don't have to change the model itself. It will just work.\n",
    "\n",
    "Next, let's look at the step method. Basically, here we let all againts play by calling the `step` method on all agents.\n",
    "\n",
    "```python\n",
    "def step(self):\n",
    "    '''Advance the model by one step.'''\n",
    "    self.agents.do(\"step\")\n",
    "```\n",
    "\n",
    "\n",
    "We can now run the model and get the scores out.\n",
    "\n",
    "```python\n",
    "scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "```\n",
    "Here we iterate over all agents, and use magic attributes to get the class name and the points accumulated over playing against all other strategies. Next, we sort this list in place based on the number of points, and sort it in reverse order.\n",
    "\n",
    "```python\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AxelrodModel(200)\n",
    "model.step()\n",
    "\n",
    "scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for entry in scores:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: adding a random agent\n",
    "\n",
    "The strategies we have been looking at so far are deterministic. Let's make this story a bit more complicated. Below, implement an additional strategy whose moves are random with an equal chance of either cooperate or defect. How does this change the results? If you rerun this model multiple times, what do you see? Why does this happen?\n",
    "\n",
    "*tip: it is a best practice in MESA to use ``self.random`` on any instance of a Mesa agent or model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random(AxelrodAgent):\n",
    "    \n",
    "    def move(self):\n",
    "        ...\n",
    "        \n",
    "    def reset(self):\n",
    "        ...\n",
    "    \n",
    "for _ in range(10):\n",
    "    model = AxelrodModel(200)\n",
    "    model.step()\n",
    "\n",
    "    scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    for entry in scores:\n",
    "        print(entry)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pseudo random number generation\n",
    "\n",
    "By adding an agent which plays a random move, we introduce randomness in the outcomes of the model. Every time we run the model, the payoffs received by each strategy will be slightly different. This might create all kinds of issues. For example, what if you have an error in your code that only occurs under very specific conditions. How can you ensure that these conditions occur when debugging if randomness plays a role? Or, how can we draw conclusions from results that are not deterministic?\n",
    "\n",
    "Randomness is intrinsic to virtually all agent based models. Computers don't actually produce real random numbers, but rather rely on deterministic algorithms that produce numbers that appear very close to random. Such algorithms are known as pseudo-random number generators. As long as we know the initial state of this algorithm, we can reproduce the exact same random numbers. If you want to know more, the [Wikipedia entry on Random Number Generation](https://en.wikipedia.org/wiki/Random_number_generation) is a good starting point. So how can we control this state?\n",
    "\n",
    "It is here that the ``seed`` argument comes in. Remember our model ``__init__`` function had ``seed`` as an optional keyword argument set to ``None`` by default. By providing a specific value, we can actually make the random numbers deterministic. Have a look at the code below to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123456789)\n",
    "[random.random() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123456789)\n",
    "[random.random() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting seed to the same value in both code blocks, we start the generation of random numbers from the same initial condition. Thus, the random numbers are the same. If seed is set to None, the computer will look at the time and use this as initial condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Noise\n",
    "\n",
    "In the foregoing, we have explored how well different strategies for playing the iterated prisoners dilemma perform assuming that there is no noise. That is, the moves of agents are executed as intended. Next, let's complicate the situation. What happens if there is a small chance that an agent makes the opposite move from what it intended to do?\n",
    "\n",
    "For this, we can adapt the model itself. If you have implemented the strategies smartly, there is no need to change anything in the strategy classes. Modify the model in the following way:\n",
    "* There is a user specifiable probability of making the opposite move. This probability is constant for all agents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxelrodModelWithNoise(Model):\n",
    "    \"\"\"A model with some number of agents.\"\"\"\n",
    "    def __init__(self, ..., seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "        self.payoff_matrix = {\n",
    "            (Move.COOPERATE, Move.COOPERATE):(2, 2),\n",
    "            (Move.COOPERATE, Move.DEFECT): (0, 3),\n",
    "            (Move.DEFECT, Move.COOPERATE): (3, 0),\n",
    "            (Move.DEFECT, Move.DEFECT): (1, 1)\n",
    "        }\n",
    "        \n",
    "        # Create agents\n",
    "        ...\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Advance the model by one step.\"\"\"\n",
    "        self.agents.do(\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AxelrodModelWithNoise(200, noise_level=0.1)\n",
    "model.step()\n",
    "\n",
    "scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for entry in scores:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different levels of noise, ranging from 1% to 10%, how does this affect the ranking of the strategies?\n",
    "\n",
    "you can use ``np.linspace`` to generate a range of evenly spaced values between 0.01 and 0.1. You can use the above code for printing the scores of a given run to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_level in np.linspace(0.01, 0.1, 10):\n",
    "    model = AxelrodModelWithNoise(200, noise_level=noise_level)\n",
    "    model.step()\n",
    "\n",
    "    print(noise_level)\n",
    "    scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    for entry in scores:\n",
    "        print(entry)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: adding evolutionary dynamics\n",
    "\n",
    "Up till now, we have run the model for only one step. That is all agents play against each other only once. Let's make the model more dynamic by adding an evolutionary component to it. We start by generating *M* agents of each strategy. These agents play against one another as before. Next, after each step, we tally up the total scores achieved by each strategy. We create a new population, proportional to how well each strategy performed. Over multiple steps, badly performing strategies will die out. However, with changing proportions of the different strategies, how well each strategy is performing will also change. Can you predict which strategies will come to dominate this population?\n",
    "\n",
    "1. Implement the ``build_population`` method which creates a population given a dictionary with proportions\n",
    "2. Calculate the new proportions as part of ``step``\n",
    "\n",
    "*hint: check the agentset documentation and look at groupby operations which are very similar to how this works in pandas*\n",
    "\n",
    "To help in keeping track of the changing proportions over time, I have added a small piece of code. In the ``__init__`` we create an attribute with the scores. This attribute is a dictionary with a list for each class of agents. In ``step`` we append the new proportions to these lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class EvolutionaryAxelrodModel(Model):\n",
    "    \"\"\"A model with some number of agents.\"\"\"\n",
    "    def __init__(self, num_agents, n, seed=None, noise_level=0):\n",
    "        super().__init__(seed=seed)\n",
    "        self.payoff_matrix = {\n",
    "            (Move.COOPERATE, Move.COOPERATE):(2, 2),\n",
    "            (Move.COOPERATE, Move.DEFECT): (0, 3),\n",
    "            (Move.DEFECT, Move.COOPERATE): (3, 0),\n",
    "            (Move.DEFECT, Move.DEFECT): (1, 1)\n",
    "        }\n",
    "        \n",
    "        self.population_size = len(AxelrodAgent.__subclasses__())*num_agents\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "        strategies = AxelrodAgent.__subclasses__()\n",
    "        num_strategies = len(strategies)\n",
    "        proportions = {agent_class:1/num_strategies for agent_class in strategies}\n",
    "\n",
    "        self.num_iterations = n\n",
    "        self.scores = defaultdict(list)\n",
    "        for agent_class in strategies:\n",
    "            self.scores[agent_class].append(proportions[agent_class])        \n",
    "        \n",
    "        self.initial_population_size = num_agents * num_strategies\n",
    "        self.build_population(proportions)\n",
    "\n",
    "\n",
    "    def remove_population(self):\n",
    "        for agent in list(self._agents.keys()):\n",
    "            agent.remove()\n",
    "\n",
    "    \n",
    "    def build_population(self, proportions):\n",
    "        for agent_class, proportion in proportions.items():\n",
    "            num_agents = round(self.initial_population_size*proportion)\n",
    "\n",
    "            for _ in range(num_agents):\n",
    "                a = agent_class(self, self.num_iterations, noise_level=noise_level)\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Advance the model by one step.\"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.step()\n",
    "\n",
    "        # calculate scores per class of agents\n",
    "        ...\n",
    "        \n",
    "        # normalize scores on unit interval\n",
    "        ...  # proportions should be dict\n",
    "        \n",
    "        # keep track of proportions over the generations\n",
    "        for agent_class in AxelrodAgent.__subclasses__():\n",
    "            self.scores[agent_class].append(proportions[agent_class])\n",
    "        \n",
    "        self.remove_population()\n",
    "        self.build_population(proportions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model with 10 agents per strategy and play 200 rounds of the game. Next run the model for 50 steps and visualize how the relative proportions of the different strategies evolve over the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvolutionaryAxelrodModel(10, 200)\n",
    "\n",
    "for _ in range(50):\n",
    "    model.step()\n",
    "\n",
    "# visualizing results using matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "for k, v in model.scores.items():\n",
    "    ax.plot(v, label=k.__name__)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Evolution with noise\n",
    "\n",
    "Building on the previous two versions of the model, as a final step we are going to explore how noise affects the evolutionary dynamics. To do this, you use the Evolutionary model from the previous step but vary `noise_level`. Again, explore the dynamics of this model for 50 steps over noise levels ranging from 1% to 10%. What do you see? Can you explain what is happening in the model? Are you surprised by these results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for noise_level in np.linspace(0.01, 0.1, 10):\n",
    "    model = EvolutionaryAxelrodModel(10, 200, noise_level=noise_level)\n",
    "\n",
    "    for _ in range(50):\n",
    "        model.step()\n",
    "\n",
    "    # visualizing results using matplotlib\n",
    "    fig, ax = plt.subplots()\n",
    "    for k, v in model.scores.items():\n",
    "        ax.plot(v, label=k.__name__)\n",
    "    ax.legend()\n",
    "    ax.set_title(noise_level)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
